---
title: "Bixverse R <> Rust interface"
vignette: >
  %\VignetteIndexEntry{Bixverse R <> Rust interface}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
knitr:
  opts_chunk:
    collapse: true
    comment: '#>'
---

## Using the Rust functions in more general R code

`bixverse` has a number of R <> Rust functions that might be useful more
generally. This vignette is there to explore how to use these functions more
general. This vignette will show you to use:

* Correlations (via `rs_cor()`).
* Covariance (via `rs_covariance()`).
* Covariance to correlation (via `rs_cov2cor()`).
* Distance calculations (vs `rs_dist()`).
* Correlations between two matrices (via `rs_cor2()`).
* Calculate the mutual information between columns (via `rs_mutual_information()`)
* Calculations of set similarities between (via `rs_set_similarity_list()` or
`rs_set_similarity()`).

```{r}
#| label: setup
library(bixverse)
```

This vignette will explore the interfaces in `bixverse` to underlying Rust
code you can leverage to make your code faster. But be careful... Any function
with `rs_` can cause panics when misused.

### Correlations, co-variance, and cosine distance

A typical thing one uses a lot are correlations, co-variance and cosine 
distances. Let's look at how to use `bixverse` to calculate these. 

```{r}
#| label: generate random data

no_rows <- 500L
no_cols <- 500L

set.seed(10101L)

random_data <- matrix(
  data = rnorm(no_rows * no_cols),
  ncol = no_cols,
  nrow = no_rows
)
```

#### Pearson's correlation

Let's look at Pearson's correlations.

```{r}
#| label: pearson correlations

r_pearsons_res <- cor(random_data)

rust_pearsons_res <- rs_cor(random_data, spearman = FALSE)

# tiny numerical differences do exist
all.equal(r_pearsons_res, rust_pearsons_res, tolerance = 1e-15)
```

Speed differences:

```{r}
#| label: pearson correlations (speed)
#| eval: !expr requireNamespace("microbenchmark", quietly = TRUE)

microbenchmark::microbenchmark(
  r = cor(random_data),
  rust = rs_cor(random_data, spearman = FALSE),
  times = 50L
)
```

#### Spearman's correlations

Let's look at Spearman's correlations...

```{r}
#| label: spearman correlations

r_spearman_res <- cor(random_data, method = "spearman")

rust_spearman_res <- rs_cor(random_data, spearman = TRUE)

# tiny numerical differences do exist
all.equal(r_spearman_res, rust_spearman_res, tolerance = 1e-15)
```

Speed differences:

```{r}
#| label: spearman correlations (speed)
#| eval: !expr requireNamespace("microbenchmark", quietly = TRUE)

microbenchmark::microbenchmark(
  r = cor(random_data, method = "spearman"),
  rust = rs_cor(random_data, spearman = TRUE),
  times = 50L
)
```

#### Covariance

And covariance calculations

```{r}
#| label: covariance

r_covar_res <- cov(random_data)

rust_covar_res <- rs_covariance(random_data)

# tiny numerical differences do exist
all.equal(r_covar_res, rust_covar_res, tolerance = 1e-15)
```

Speed differences:

```{r}
#| label: covariance (speed)
#| eval: !expr requireNamespace("microbenchmark", quietly = TRUE)

microbenchmark::microbenchmark(
  r = cov(random_data),
  rust = rs_covariance(random_data),
  times = 50L
)
```

Let's transform covariance to correlations

```{r}
#| label: covariance to correlation

r_cor_from_covar <- cov2cor(r_covar_res)

rust_cor_from_covar <- rs_cov2cor(rust_covar_res)

# tiny numerical differences do exist
all.equal(r_cor_from_covar, rust_cor_from_covar, tolerance = 1e-15)
```

How much faster is Rust here? (This is where we observe the least improvement.)

```{r}
#| label: covariance to correlation (speed)
#| eval: !expr requireNamespace("microbenchmark", quietly = TRUE)

microbenchmark::microbenchmark(
  r = cov2cor(r_covar_res),
  rust = rs_cov2cor(rust_covar_res),
  times = 50L
)
```

#### Correlations between two matrices

We also provide interface to do correlations between two sets of data

```{r}
#| label: generate a second random data

no_rows_2 <- 500L
no_cols_2 <- 400L

set.seed(23L)

random_data_2 <- matrix(
  data = rnorm(no_rows_2 * no_cols_2, mean = 2, sd = 2),
  ncol = no_cols_2,
  nrow = no_rows_2
)
```

Let's test the correlations between the two matrices

```{r}
#| label: R vs Rust second corelations
r_cor_2_matrices <- cor(random_data, random_data_2)

rust_cor_2_matrices <- rs_cor2(random_data, random_data_2, spearman = FALSE)

# The small precision differences are further propagated in here
all.equal(r_cor_2_matrices, rust_cor_2_matrices, tolerance = 1e-14)
```

And speed between Rust and R

```{r}
#| label: correlations between two matrices
#| eval: !expr requireNamespace("microbenchmark", quietly = TRUE)

microbenchmark::microbenchmark(
  r = cor(random_data, random_data_2),
  rust = rs_cor2(random_data, random_data_2, spearman = FALSE),
  times = 50L
)
```

### Distance metrics 

`bixverse` also includes various distance metrics. Let's have a look at very
common ones.

#### Euclidean distance

Let's check out the very classical Euclidean distance.

```{r}
#| label: euclidean distance

r_euclidean_distance <- as.matrix(
  dist(
    random_data, 
    method = "euclidean"
  )
)

rs_euclidean_distance <- rs_dist(
  t(random_data), # dist() calculates row-wise distances
  distance_type = "euclidean"
)

all.equal(
  r_euclidean_distance, 
  rs_euclidean_distance, 
  tolerance = 1e-14,
  check.attributes = FALSE
)
```

Again, Rust is much faster...

```{r}
#| label: euclidean distance speed
#| eval: !expr requireNamespace("microbenchmark", quietly = TRUE)

# to remove any overhead, we do not do the matrix transformation
# or the transpositation. due to the equal dimensions, it will
# not matter
microbenchmark::microbenchmark(
  r = dist(random_data, method = "euclidean"),
  rust = rs_dist(random_data, distance_type = "euclidean"),
  times = 50L
)
```

#### Other distance metrics

Let's test other distance metrics...

**Manhattan distance**

We observe again equivalence of Rust and R results.

```{r}
#| label: manhattan distance

r_manhattan_distance <- as.matrix(
  dist(
    random_data, 
    method = "manhattan"
  )
)

rs_manhattan_distance <- rs_dist(
  t(random_data), # dist() calculates row-wise distances
  distance_type = "manhattan"
)

all.equal(
  r_manhattan_distance, 
  rs_manhattan_distance, 
  tolerance = 1e-14,
  check.attributes = FALSE
)
```

And speed ups in Rust.

```{r}
#| label: manhattan distance speed
#| eval: !expr requireNamespace("microbenchmark", quietly = TRUE)

# to remove any overhead, we do not do the matrix transformation
# or the transpositation. due to the equal dimensions, it will
# not matter
microbenchmark::microbenchmark(
  r = dist(random_data, method = "manhattan"),
  rust = rs_dist(random_data, distance_type = "manhattan"),
  times = 50L
)
```

**Canberra distance**

Equivalence observed...

```{r}
#| label: canberra distance

r_canberra_distance <- as.matrix(
  dist(
    random_data, 
    method = "canberra"
  )
)

rs_canberra_distance <- rs_dist(
  t(random_data), # dist() calculates row-wise distances
  distance_type = "canberra"
)

all.equal(
  r_canberra_distance, 
  rs_canberra_distance, 
  tolerance = 1e-14,
  check.attributes = FALSE
)
```

... and speed-ups.

```{r}
#| label: canberra distance speed
#| eval: !expr requireNamespace("microbenchmark", quietly = TRUE)

# to remove any overhead, we do not do the matrix transformation
# or the transpositation. due to the equal dimensions, it will
# not matter
microbenchmark::microbenchmark(
  r = dist(random_data, method = "canberra"),
  rust = rs_dist(random_data, distance_type = "canberra"),
  times = 50L
)
```

### Mutual information 

Another quantity you might be interested in is the mutual information between
between your variables of interest. `bixverse` also provides a Rust-powered 
interface into very rapid calculations in this regard.

```{r}
#| label: generate new smaller data

# we are going to create a reduced set for the benchmark to finish 
# in a reasonable time
set.seed(246L)

nrows <- 100
ncols <- 100

mat <- matrix(data = rnorm(nrows * ncols), nrow = nrows, ncol = ncols)
rownames(mat) <- sprintf("sample_%i", 1:nrows)
colnames(mat) <- sprintf("feature_%i", 1:ncols)
```

To run the function you can just use the following code. The Rust version
will default to sqrt(nrow())

```{r}
#| label: mutual information calculations - equal width
#| eval: !expr requireNamespace("infotheo", quietly = TRUE)

rust_res_mi <- rs_mutual_info(
  mat, 
  n_bins = NULL, 
  normalise = FALSE,
  strategy = "equal_width"
)
rownames(rust_res_mi) <- colnames(rust_res_mi) <- colnames(mat)

# ensure that the same discretisation is used
infotheo_res_mi <- infotheo::mutinformation(infotheo::discretize(
  mat,
  disc = "equalwidth",
  nbins = sqrt(nrow(mat))
))

all.equal(rust_res_mi, infotheo_res_mi)
```

There is also an equal frequency strategy implemented:

```{r}
#| label: mutual information calculations - equal frequency
#| eval: !expr requireNamespace("infotheo", quietly = TRUE)

rust_res_mi <- rs_mutual_info(
  mat, 
  n_bins = NULL, 
  normalise = FALSE,
  strategy = "equal_freq"
)
rownames(rust_res_mi) <- colnames(rust_res_mi) <- colnames(mat)

# ensure that the same discretisation is used
infotheo_res_mi <- infotheo::mutinformation(infotheo::discretize(
  mat,
  disc = "equalfreq",
  nbins = sqrt(nrow(mat))
))

all.equal(rust_res_mi, infotheo_res_mi)
```

Again, the Rust implementation is way faster.

```{r}
#| label: speed differences bixverse vs. infotheo (version 1)
#| eval: !expr requireNamespace(c("infotheo", "microbenchmark"), quietly = TRUE)

microbenchmark::microbenchmark(
  infotheo = infotheo::mutinformation(infotheo::discretize(
    mat,
    disc = "equalwidth",
    nbins = sqrt(nrow(mat))
  )),
  rust = rs_mutual_info(
    mat, 
    n_bins = NULL, 
    normalise = FALSE,
    strategy = "equal_width"
  ),
  times = 50L
)
```

And with equal frequency per bin:
```{r}
#| label: speed differences bixverse vs. infotheo (version 2)
#| eval: !expr requireNamespace(c("infotheo", "microbenchmark"), quietly = TRUE)

microbenchmark::microbenchmark(
  infotheo = infotheo::mutinformation(infotheo::discretize(
    mat,
    disc = "equalfreq",
    nbins = sqrt(nrow(mat))
  )),
  rust = rs_mutual_info(
    mat, 
    n_bins = NULL, 
    normalise = FALSE,
    strategy = "equal_freq"
  ),
  times = 50L
)
```

### Set similarities

`bixverse` also provides various functions that leverage Rust to do set
similarities. Let's compare this against some R implementation. Below is a
naive R implementation to calculate the Jaccard similarities between two 
lists containing strings.

```{r}
#| label: R similarity data

jaccard_sim <- function(x, y) {
  length(intersect(x, y)) / length(union(x, y))
}

set.seed(42L)

random_sets_1 <- purrr::map(1:1000L, ~{
  paste(sample(LETTERS, sample(1:20, 1)))
})

random_sets_2 <- purrr::map(1:500L, ~{
  paste(sample(LETTERS, sample(1:20, 1)))
})
```

Let's start with a naive [purrr](https://purrr.tidyverse.org) approach. 
The problem obviously will benefit from some form of parallel processing, but 
for completeness, let's run this version, too.

```{r}
#| label: set similarity in R (purrr)
#| eval: !expr requireNamespace(c("future", "tictoc", "furrr"), quietly = TRUE)

tictoc::tic()

r_results <- purrr::map(random_sets_1, \(x) {
  purrr::map_dbl(random_sets_2, \(y) {jaccard_sim(x, y)})
}, .progress = TRUE)

similarity_matrix <- matrix(
  data = unlist(r_results), 
  nrow = length(random_sets_1), 
  ncol = length(random_sets_2),
  byrow = TRUE
)
tictoc::toc()
```

Let's try to parallelise this vi [furrr](https://furrr.futureverse.org) and 
evaluate how much we can improve this.

```{r}
#| label: set similarity in R (furrr)
#| eval: !expr requireNamespace(c("future", "tictoc", "furrr"), quietly = TRUE)

tictoc::tic()
future::plan(strategy = future::multisession())

r_results <- furrr::future_map(random_sets_1, \(x) {
  purrr::map_dbl(random_sets_2, \(y) {jaccard_sim(x, y)})
}, .progress = TRUE)

future::plan(strategy = future::sequential())

similarity_matrix <- matrix(
  data = unlist(r_results), 
  nrow = length(random_sets_1), 
  ncol = length(random_sets_2),
  byrow = TRUE
)
tictoc::toc()
```

Parallelisation as expected accelerates the calculations substantially. 
What about [mirai](https://mirai.r-lib.org/articles/mirai.html)? It should 
produce less overhead compared to future.

```{r}
#| label: set similarity in R (mirai)
#| eval: !expr requireNamespace(c("tictoc", "mirai"), quietly = TRUE)

tictoc::tic()
mirai::daemons(4)

r_results_mirai <- mirai::mirai_map(random_sets_1, \(x, sets_2) {
  purrr::map_dbl(sets_2, \(y) {
    length(intersect(x, y)) / length(union(x, y))
  })
}, .args = list(sets_2 = random_sets_2))[]

similarity_matrix <- matrix(
  data = unlist(r_results_mirai), 
  nrow = length(random_sets_1), 
  ncol = length(random_sets_2),
  byrow = TRUE
)

mirai::daemons(0)
tictoc::toc()
```

That's already a bit better. Maybe we can optimise the Jaccard similarity 
function further and accelerate this even more.

```{r}
#| label: set similarity in R (mirai, optimised)
#| eval: !expr requireNamespace(c("tictoc", "mirai"), quietly = TRUE)

tictoc::tic()
mirai::daemons(4)

r_results_mirai <- mirai::mirai_map(random_sets_1, \(x, sets_2) {
  purrr::map_dbl(sets_2, \(y) {
    # Optimised function
    intersection <- length(intersect(x, y))
    union_size <- length(x) + length(y) - intersection
    intersection / union_size
  })
}, .args = list(sets_2 = random_sets_2))[]

similarity_matrix <- matrix(
  data = unlist(r_results), 
  nrow = length(random_sets_1), 
  ncol = length(random_sets_2),
  byrow = TRUE
)

mirai::daemons(0)
tictoc::toc()
```

This is already much better... Pending on your system you might be seeing a 4x
increase in speed compared to the naive purrr version. However, let's compare 
against the Rust version implemented in `bixverse`:

```{r}
#| label: bixverse jaccard similarity function

tictoc::tic()
rust_res <- rs_set_similarity_list(
  s_1_list = random_sets_1,
  s_2_list = random_sets_2,
  overlap_coefficient = FALSE
)
tictoc::toc()

all.equal(similarity_matrix, rust_res, tolerance = 1e-15)
```

Yep, that is MUCH faster... These are some of the functions that are exposed
in `bixverse` and can be integrated into other workflows. There are many more
highly specialised functions that can be used in other workflows. It's best
to explore the package and the underlying code base to identify all of these
functions.
